{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   @@@@@@@     @      @    @  @   #\n",
    "#   @  @  @    @ @      @  @       #\n",
    "#      @  @   @   @      @@    @   #\n",
    "#      @     @@@@@@@    @  @   @   #\n",
    "#      @    @       @  @    @  @   #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет, в этой практике мы с вами применим наши знания по PySpark и постараемся изучить что-то новое в процессе выполнения.\n",
    "<br>В занятии используется датасет собранный на основе данных <a href=\"https://www.kaggle.com/chicago/chicago-taxi-rides-2016\">Chicago Taxi Rides 2016</a>\n",
    "<br>Полная <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\">документация PySpark</a>.\n",
    "<br>Схема данны:\n",
    "<br>|-- taxi_id = идентификатор таксиста\n",
    "<br>|-- trip_start_timestamp = время начала поездки\n",
    "<br>|-- trip_end_timestamp = время окончания поездки\n",
    "<br>|-- trip_seconds = время длительности поездки в секундах\n",
    "<br>|-- trip_miles = мили проиденные во время поездки\n",
    "<br>|-- fare = транспортные расходы\n",
    "<br>|-- tips = назначенные чаевые\n",
    "<br>|-- trip_total = общая стоимость поездки\n",
    "<br>|-- payment_type = тип оплаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySparkTasks').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"GMT+3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Al-PC:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkTasks</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x6349a88>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте <a href=\"https://github.com/AlexKbit/stepik-ds-course/raw/master/Week3/spark-tasks/taxi_data.parquet\">taxi_data.parquet</a> и загрузите используя <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">SparkAPI</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  spark.read.parquet('taxi_data.parquet') #Ваш код загрузки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№1 Посчитайте количество загруженных строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2540712"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # Число строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------+----------+-----+-----+----------+------------+\n",
      "|taxi_id|trip_start_timestamp| trip_end_timestamp|trip_seconds|trip_miles| fare| tips|trip_total|payment_type|\n",
      "+-------+--------------------+-------------------+------------+----------+-----+-----+----------+------------+\n",
      "|   5240| 2016-12-15 23:45:00|2016-12-16 00:00:00|         900|       2.5|10.75| 2.45|      14.7| Credit Card|\n",
      "|   1215| 2016-12-12 07:15:00|2016-12-12 07:15:00|         240|       0.4|  5.0|  3.0|       9.5| Credit Card|\n",
      "|   3673| 2016-12-16 16:30:00|2016-12-16 17:00:00|        2400|      10.7| 31.0|  0.0|      31.0|        Cash|\n",
      "|   5400| 2016-12-16 08:45:00|2016-12-16 09:00:00|         300|       0.0| 5.25|  2.0|      7.25| Credit Card|\n",
      "|   1257| 2016-12-03 18:45:00|2016-12-03 18:45:00|         360|       0.3|  5.0|  0.0|       5.0|        Cash|\n",
      "|   4666| 2016-12-30 18:00:00|2016-12-30 18:45:00|        2400|      18.2|46.75|10.15|      61.4| Credit Card|\n",
      "|   5998| 2016-12-16 07:15:00|2016-12-16 07:45:00|        1800|      18.4| 45.0|11.25|     56.25| Credit Card|\n",
      "|   2538| 2016-12-31 17:15:00|2016-12-31 17:15:00|         540|       0.0| 6.75|  0.0|      7.75|        Cash|\n",
      "|   6594| 2016-12-17 12:00:00|2016-12-17 12:00:00|         153|      0.47|  4.5|  1.0|       6.0| Credit Card|\n",
      "|   7864| 2016-12-03 19:45:00|2016-12-03 20:00:00|         780|       2.4| 10.5|  2.0|      12.5| Credit Card|\n",
      "|    400| 2016-12-06 15:30:00|2016-12-06 16:30:00|        3120|      10.9|31.75| 7.93|     40.18| Credit Card|\n",
      "|   6482| 2016-12-09 09:30:00|2016-12-09 09:30:00|         660|      1.32| 7.75|  2.0|     10.25| Credit Card|\n",
      "|   5856| 2016-12-29 18:45:00|2016-12-29 19:00:00|         300|       0.0| 5.25|  0.0|      6.75|        Cash|\n",
      "|   7211| 2016-12-12 22:30:00|2016-12-12 23:00:00|        1320|      14.1| 36.0|  0.0|      41.0|        Cash|\n",
      "|   1094| 2016-12-10 21:00:00|2016-12-10 21:00:00|         540|       1.4| 6.75|  4.0|     10.75| Credit Card|\n",
      "|   6591| 2016-12-22 22:45:00|2016-12-22 23:00:00|         480|       1.7| 7.75|  0.0|      7.75|        Cash|\n",
      "|   6514| 2016-12-30 11:00:00|2016-12-30 11:00:00|         480|       1.5|  7.5|  0.0|       7.5|        Cash|\n",
      "|   8267| 2016-12-17 04:15:00|2016-12-17 04:15:00|         360|       1.9| 7.75|  4.0|     13.25| Credit Card|\n",
      "|   8002| 2016-12-19 12:15:00|2016-12-19 12:15:00|         300|       0.9|  6.0|  2.0|       8.5| Credit Card|\n",
      "|   2718| 2016-12-19 10:15:00|2016-12-19 10:45:00|        1920|       5.6|18.75|  0.0|     18.75|        Cash|\n",
      "+-------+--------------------+-------------------+------------+----------+-----+-----+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим схему данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- taxi_id: integer (nullable = true)\n",
      " |-- trip_start_timestamp: timestamp (nullable = true)\n",
      " |-- trip_end_timestamp: timestamp (nullable = true)\n",
      " |-- trip_seconds: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- trip_total: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№2 Чему равна корреляция и ковариация между длиной маршрута и ценой за поездку? Ответ округлите до 5 знаков после запятой.\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.corr\">corr</a> & <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cov\">cov</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44816"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.corr('trip_miles','trip_total'), 5) # Ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.96914"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.cov('trip_miles','trip_total'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№3 Найдите количество, среднее, cреднеквадратическое отклонение, минимум и максимум для длины маршрута и цены за поездку? Ответ округлите до 1 знака после запятой. Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe\">describe</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|        trip_miles|        trip_total|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           2540677|           2540672|\n",
      "|   mean|3.0005873828090266|15.913560215564042|\n",
      "| stddev|  5.25716922943536|30.546699217618237|\n",
      "|    min|               0.0|               0.0|\n",
      "|    max|             900.0|           9276.69|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['trip_miles','trip_total']).show()# Ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo огруглить https://stackoverflow.com/questions/47046827/trouble-with-pyspark-round-function\n",
    "# type(df.describe(['trip_miles','trip_total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№4 Найдите самый НЕ популярный вид оплаты.\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\">groupBy</a> <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">orderBy</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(payment_type='Way2ride', count=3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(['payment_type']).count().orderBy('count').collect()[0]# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№5 Найдите идентификатор таксиста выполнившего наибольшее число заказов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(taxi_id=316, count=2225)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('taxi_id').count().orderBy('count', ascending=False).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№6 Чему равна средняя цена среди поездок, оплаченных наличными? Ответ округлите до 5 знака.\n",
    "<br> Подробней <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where\">where</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = df.filter(\"payment_type == 'Cash'\")\\\n",
    "    .agg({\"trip_total\": \"avg\"})\\\n",
    "    .withColumnRenamed('avg(trip_total)', 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|               avg|round(avg, 5)|\n",
      "+------------------+-------------+\n",
      "|12.035261470840307|     12.03526|\n",
      "+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ndf.select(\"*\", round(col('avg'),5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.filter(df['payment_type'] == 'Cash').agg(avg(col(\"trip_total\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№7 Сколько таксистов проехало больше 1000 миль за все время выполнения заказов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2860"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(col('taxi_id'))\\\n",
    "    .agg({'trip_miles':'sum'})\\\n",
    "    .withColumnRenamed('sum(trip_miles)', 'sum')\\\n",
    "    .filter(col('sum') > 1000)\\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№8 Сколько миль проехал пассажир в самой долгой поездке? (Ответ округлите до целого)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|trip_miles|\n",
      "+----------+\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "|       0.0|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('trip_seconds', ascending=False)[['trip_miles']].show(3) # Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№9 Каков средний заработок всех таксистов? Ответ округлите до 5-ого знака.\n",
    "<br>Отсеките неизвестные машины (не определенный taxi_id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         avg(sum)|\n",
      "+-----------------+\n",
      "|8218.856265256312|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['taxi_id'].isNull() != True)\\\n",
    "    .groupBy('taxi_id')\\\n",
    "    .agg({'trip_total' : 'sum'})\\\n",
    "    .withColumnRenamed('sum(trip_total)', 'sum')\\\n",
    "    .agg({'sum':'avg'})\\\n",
    "    .show()\n",
    "#.collect()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№10 Сколько поездок начиналось в самый загруженный час?\n",
    "<br>Используйте функцию <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.hour\">hour</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+------+\n",
      "|hour(trip_start_timestamp)| count|\n",
      "+--------------------------+------+\n",
      "|                        18|181127|\n",
      "|                        19|173779|\n",
      "|                        17|169886|\n",
      "|                        16|156519|\n",
      "|                        20|146602|\n",
      "+--------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(hour('trip_start_timestamp')).count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№11 Сколько поездок началось во второй четверти дня?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538737"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((hour(df['trip_start_timestamp']) >= 6) & (hour(df['trip_start_timestamp']) < 12)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№12 Найдите топ три даты, в которые было суммарно больше всего чаевых? (Чаевые выдаются после совершения поездки)\n",
    "<br> Ожидаемый формат дат YYYY-MM-DD\n",
    "<br>Вам может понадобится конвертация типов <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast\">cast</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------------+\n",
      "|to_date(`trip_start_timestamp`)|         sum(tips)|\n",
      "+-------------------------------+------------------+\n",
      "|                     2016-11-03|109894.00000000015|\n",
      "|                     2016-11-09| 106329.8699999999|\n",
      "|                     2016-11-16|100253.60000000037|\n",
      "|                     2016-12-01|  98208.2300000001|\n",
      "|                     2016-11-10| 97594.29000000063|\n",
      "+-------------------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(to_date(df['trip_start_timestamp'])).agg({\"tips\": \"sum\"}).orderBy('sum(tips)', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№13 Сколько было заказов в дату с наибольшим спросом?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----+\n",
      "|to_date(`trip_start_timestamp`)|count|\n",
      "+-------------------------------+-----+\n",
      "|                     2016-11-03|61259|\n",
      "|                     2016-12-16|59137|\n",
      "|                     2016-12-09|58583|\n",
      "|                     2016-12-15|57084|\n",
      "|                     2016-11-04|56800|\n",
      "+-------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(to_date(df['trip_start_timestamp'])).count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузите данные о марках машин из датасета <a href=\"https://github.com/AlexKbit/stepik-ds-course/raw/master/Week3/spark-tasks/taxi_cars_data.parquet\">taxi_cars_data.parquet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_car = spark.read.parquet('taxi_cars_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|taxi_id|          car_model|\n",
      "+-------+-------------------+\n",
      "|   1159|       Toyota Prius|\n",
      "|   7273|Ford Crown Victoria|\n",
      "|   2904|        Honda Civic|\n",
      "|   3210|        Ford Fusion|\n",
      "|   2088|       Toyota Camry|\n",
      "|   4821|Ford Crown Victoria|\n",
      "|   2069|    Hyundai Elantra|\n",
      "|   6240|       Toyota Camry|\n",
      "|    296|     Hyundai Sonata|\n",
      "|   2136|Ford Crown Victoria|\n",
      "|   1436|     Toyota Corolla|\n",
      "|   1090|     Toyota Corolla|\n",
      "|   7711|        Lincoln MKZ|\n",
      "|   1572|Ford Crown Victoria|\n",
      "|   3959|     Chevrolet Aveo|\n",
      "|   5645|  Chevrolet Lacetti|\n",
      "|   5149|            Audi A7|\n",
      "|   6366|     Hyundai Sonata|\n",
      "|    451|     Hyundai Accent|\n",
      "|   1669|           Kia Ceed|\n",
      "+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_car.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№14 Какая марка машины самая распрастранненая среди таксистов?\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split\">split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----+\n",
      "|split(car_model,  , -2)[0]|count|\n",
      "+--------------------------+-----+\n",
      "|                      Ford| 1484|\n",
      "|                   Hyundai|  792|\n",
      "|                    Toyota|  691|\n",
      "|                 Chevrolet|  473|\n",
      "|                       Kia|  265|\n",
      "+--------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_car.groupBy(split(df_car['car_model'], ' ')[0])\\\n",
    ".count().orderBy('count', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№15 Сколько раз и какая модель машин чаще всего встречается в поездках?\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\">join</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|          car_model| count|\n",
      "+-------------------+------+\n",
      "|Ford Crown Victoria|388682|\n",
      "|     Hyundai Accent|150764|\n",
      "|           Kia Ceed|143649|\n",
      "|     Hyundai Sonata|141570|\n",
      "|        Ford Mondeo|135466|\n",
      "+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df_car, df.taxi_id == df_car.taxi_id).groupBy(df_car['car_model'])\\\n",
    ".count().orderBy('count', ascending=False).show(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почувствуй силу сжатия! сохрани DataFrame в csv и сравни размеры файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код с coalesce(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузите данные из csv и проверьте типы методом printSchema()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код с printSchema() для DataFrame из csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забудьте посетить SparkUI и изучить историю ваших задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Al-PC:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkTasks</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x62cf2c8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
